{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3be0837",
   "metadata": {},
   "source": [
    "# Azure OpenAI Em Dash Pipeline - Data Generation\n",
    "\n",
    "This notebook contains the **data generation pipeline** for the em dash analysis project. It generates stories using different AI models, extracts em dash sentences, and creates paraphrases.\n",
    "\n",
    "## Pipeline Overview\n",
    "1. **Story Generation**: Generate stories using GPT-35-Turbo, GPT-4, and GPT-4.1\n",
    "2. **Em Dash Extraction**: Extract sentences containing em dashes from generated stories\n",
    "3. **Paraphrase Generation**: Create paraphrases that remove em dashes\n",
    "4. **Data Export**: Save all generated data to JSON files for analysis\n",
    "\n",
    "## Output Files\n",
    "- `story_analyses_YYYYMMDD_HHMMSS.json` - Generated stories with analysis\n",
    "- `em_dash_sentences_YYYYMMDD_HHMMSS.json` - Extracted em dash sentences\n",
    "- `paraphrase_results_YYYYMMDD_HHMMSS.json` - Generated paraphrases\n",
    "- `complete_analysis_YYYYMMDD_HHMMSS.json` - Combined pipeline results\n",
    "\n",
    "---\n",
    "\n",
    "**Related Notebooks:**\n",
    "- `evaluation_analysis.ipynb` - Analysis and visualization of generated data\n",
    "- `mystery_clean.ipynb` - Original development version with all sections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3d31ad",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7aea29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install openai python-dotenv tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4335f21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "from openai import AzureOpenAI\n",
    "from dotenv import load_dotenv\n",
    "import tiktoken\n",
    "from datetime import datetime\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce070ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure Azure OpenAI client\n",
    "api_key = \"6514FollowMySubstackBlog2745\"\n",
    "api_version = \"\"\n",
    "azure_endpoint = \"msukhareva.substack.com\"\n",
    "\n",
    "client = AzureOpenAI(\n",
    "    api_key=api_key,\n",
    "    api_version=api_version,\n",
    "    azure_endpoint=azure_endpoint\n",
    ")\n",
    "\n",
    "# Available models for analysis\n",
    "available_models = [\"gpt-35-turbo\", \"gpt-4\", \"gpt-4.1\"]\n",
    "\n",
    "print(\"\u2705 Azure OpenAI client configured successfully!\")\n",
    "print(f\"\ud83d\udcca Available models: {available_models}\")\n",
    "print(f\"\ud83c\udf10 Endpoint: {azure_endpoint}\")\n",
    "print(f\"\ud83d\udcdd API Version: {api_version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46c1996",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the connection\n",
    "try:\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-35-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": \"Hello! Can you confirm that the Azure OpenAI connection is working?\"}\n",
    "        ],\n",
    "        max_tokens=100,\n",
    "        temperature=0.7\n",
    "    )\n",
    "    \n",
    "    print(\"\u2705 Connection successful!\")\n",
    "    print(f\"Response: {response.choices[0].message.content}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\u274c Connection failed: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a797932c",
   "metadata": {},
   "source": [
    "## 2. Core Pipeline Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709d814f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenizer_for_model(model_name):\n",
    "    \"\"\"Get the appropriate tokenizer for the model\"\"\"\n",
    "    if model_name in [\"gpt-4\", \"gpt-4.1\"]:\n",
    "        return tiktoken.encoding_for_model(\"gpt-4\")\n",
    "    elif model_name == \"gpt-35-turbo\":\n",
    "        return tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "    else:\n",
    "        return tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "\n",
    "def count_tokens(text, model_name):\n",
    "    \"\"\"Count tokens in text using the appropriate tokenizer for the model\"\"\"\n",
    "    try:\n",
    "        tokenizer = get_tokenizer_for_model(model_name)\n",
    "        tokens = tokenizer.encode(text)\n",
    "        return {\n",
    "            'token_count': len(tokens),\n",
    "            'text_length': len(text),\n",
    "            'model': model_name\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {'token_count': 0, 'text_length': len(text), 'model': model_name, 'error': str(e)}\n",
    "\n",
    "def count_em_dashes(text):\n",
    "    \"\"\"Count different types of dashes in text\"\"\"\n",
    "    em_dash_count = text.count('\u2014')  # Em dash (U+2014)\n",
    "    en_dash_count = text.count('\u2013')  # En dash (U+2013)\n",
    "    double_hyphen_count = text.count('--')  # Double hyphen\n",
    "    \n",
    "    total_em_dashes = em_dash_count + en_dash_count + double_hyphen_count\n",
    "    \n",
    "    return {\n",
    "        'em_dash_count': em_dash_count,\n",
    "        'en_dash_count': en_dash_count,\n",
    "        'double_hyphen_count': double_hyphen_count,\n",
    "        'total_em_dashes': total_em_dashes\n",
    "    }\n",
    "\n",
    "def analyze_story_text(story, model_name):\n",
    "    \"\"\"Comprehensive analysis of story text\"\"\"\n",
    "    token_info = count_tokens(story, model_name)\n",
    "    dash_info = count_em_dashes(story)\n",
    "    \n",
    "    word_count = len(story.split())\n",
    "    sentence_count = len([s for s in story.split('.') if s.strip()])\n",
    "    \n",
    "    return {\n",
    "        'story_text': story,\n",
    "        'model': model_name,\n",
    "        'word_count': word_count,\n",
    "        'sentence_count': sentence_count,\n",
    "        'character_count': len(story),\n",
    "        **token_info,\n",
    "        **dash_info\n",
    "    }\n",
    "\n",
    "def save_to_json(data, filename_prefix, description=\"data\"):\n",
    "    \"\"\"Save data to timestamped JSON file\"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filename = f\"{filename_prefix}_{timestamp}.json\"\n",
    "    \n",
    "    try:\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        file_size = os.path.getsize(filename)\n",
    "        file_size_kb = file_size / 1024\n",
    "        \n",
    "        print(f\"\u2705 {description} saved to: {filename}\")\n",
    "        print(f\"\ud83d\udcc1 File size: {file_size_kb:.1f} KB\")\n",
    "        \n",
    "        return filename\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\u274c Error saving {description}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "print(\"\u2705 Core pipeline functions loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db70ae70",
   "metadata": {},
   "source": [
    "## 3. Story Generation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f87c971",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random story prompt generation\n",
    "characters = [\n",
    "    \"a mysterious librarian\", \"an elderly baker\", \"a young astronaut\", \"a street musician\",\n",
    "    \"a detective\", \"a time traveler\", \"a robot\", \"a witch\", \"a pirate\", \"a teacher\",\n",
    "    \"a chef\", \"a scientist\", \"a ghost\", \"a photographer\", \"a taxi driver\"\n",
    "]\n",
    "\n",
    "settings = [\n",
    "    \"in a haunted mansion\", \"on a distant planet\", \"in a busy coffee shop\", \"during a thunderstorm\",\n",
    "    \"in an underwater city\", \"at a carnival\", \"in a secret laboratory\", \"on a moving train\",\n",
    "    \"in a magical forest\", \"at a school reunion\", \"in a small town\", \"on a deserted island\",\n",
    "    \"in the future\", \"in ancient times\", \"in a parallel universe\"\n",
    "]\n",
    "\n",
    "objects = [\n",
    "    \"a mysterious letter\", \"an old photograph\", \"a broken compass\", \"a glowing stone\",\n",
    "    \"a music box\", \"a diary\", \"a map\", \"a key\", \"a mirror\", \"a book\",\n",
    "    \"a watch that runs backwards\", \"a painting\", \"a telescope\", \"a locked box\", \"a recipe\"\n",
    "]\n",
    "\n",
    "emotions_themes = [\n",
    "    \"seeking revenge\", \"looking for love\", \"trying to solve a mystery\", \"facing their biggest fear\",\n",
    "    \"discovering a hidden talent\", \"making an important decision\", \"reuniting with an old friend\",\n",
    "    \"starting over in life\", \"protecting someone they care about\", \"learning a valuable lesson\"\n",
    "]\n",
    "\n",
    "def generate_random_prompt():\n",
    "    \"\"\"Generate a random story prompt\"\"\"\n",
    "    character = random.choice(characters)\n",
    "    setting = random.choice(settings)\n",
    "    obj = random.choice(objects)\n",
    "    theme = random.choice(emotions_themes)\n",
    "    \n",
    "    prompt = f\"Write a short story about {character} {setting} who discovers {obj} while {theme}.\"\n",
    "    return prompt\n",
    "\n",
    "def generate_ai_story(prompt, model_name):\n",
    "    \"\"\"Generate a story using the specified AI model\"\"\"\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model_name,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a creative writer. Write engaging short stories with rich descriptions and dialogue. Use natural punctuation including em dashes when appropriate for dramatic pauses or emphasis.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            max_tokens=800,\n",
    "            temperature=0.8\n",
    "        )\n",
    "        \n",
    "        # Check if response and content exist\n",
    "        if not response or not response.choices or len(response.choices) == 0:\n",
    "            return f\"Error generating story with {model_name}: Empty response from API\"\n",
    "        \n",
    "        content = response.choices[0].message.content\n",
    "        if content is None:\n",
    "            return f\"Error generating story with {model_name}: API returned None content\"\n",
    "        \n",
    "        story = content.strip()\n",
    "        if not story:\n",
    "            return f\"Error generating story with {model_name}: API returned empty content\"\n",
    "        \n",
    "        return story\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"Error generating story with {model_name}: {str(e)}\"\n",
    "\n",
    "def generate_stories_with_full_analysis():\n",
    "    \"\"\"Generate stories with all three models and analyze them\"\"\"\n",
    "    prompt = generate_random_prompt()\n",
    "    \n",
    "    print(f\"\ud83d\udcdd PROMPT: {prompt}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    analyses = []\n",
    "    \n",
    "    for model in available_models:\n",
    "        print(f\"\\n\ud83e\udd16 Generating story with {model.upper()}...\")\n",
    "        \n",
    "        story = generate_ai_story(prompt, model)\n",
    "        \n",
    "        if story.startswith(\"Error\"):\n",
    "            print(f\"\u274c {story}\")\n",
    "            continue\n",
    "            \n",
    "        analysis = analyze_story_text(story, model)\n",
    "        analysis['prompt'] = prompt\n",
    "        analyses.append(analysis)\n",
    "        \n",
    "        print(f\"\ud83d\udcca Analysis: {analysis['word_count']} words, {analysis['token_count']} tokens, {analysis['total_em_dashes']} em dashes\")\n",
    "        print(f\"\ud83d\udcd6 Story preview: {story[:100]}...\")\n",
    "        \n",
    "        time.sleep(1)  # Rate limiting\n",
    "    \n",
    "    return analyses\n",
    "\n",
    "def generate_multiple_stories_with_analysis(num_story_rounds=3):\n",
    "    \"\"\"Generate multiple rounds of stories for comprehensive analysis\"\"\"\n",
    "    print(f\"\ud83c\udfad GENERATING {num_story_rounds} ROUNDS OF STORIES\")\n",
    "    print(f\"\ud83d\udcca Total stories to generate: {num_story_rounds * len(available_models)}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    all_analyses = []\n",
    "    \n",
    "    for round_num in range(1, num_story_rounds + 1):\n",
    "        print(f\"\\n\ud83c\udfaf ROUND {round_num}/{num_story_rounds}\")\n",
    "        round_analyses = generate_stories_with_full_analysis()\n",
    "        all_analyses.extend(round_analyses)\n",
    "        \n",
    "        if round_num < num_story_rounds:\n",
    "            print(\"\\n\u23f1\ufe0f Waiting before next round...\")\n",
    "            time.sleep(2)\n",
    "    \n",
    "    print(f\"\\n\u2705 COMPLETED! Generated {len(all_analyses)} stories total\")\n",
    "    return all_analyses\n",
    "\n",
    "print(\"\u2705 Story generation functions ready!\")\n",
    "print(\"\ud83c\udfad Functions available: generate_ai_story, generate_stories_with_full_analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8757e897",
   "metadata": {},
   "source": [
    "## 4. Em Dash Extraction Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d735d853",
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_em_dash(sentence):\n",
    "    \"\"\"Check if sentence contains any type of em dash\"\"\"\n",
    "    return '\u2014' in sentence or '\u2013' in sentence or '--' in sentence\n",
    "\n",
    "def extract_sentences_with_em_dashes(text):\n",
    "    \"\"\"Extract all sentences that contain em dashes\"\"\"\n",
    "    # Split text into sentences (simple approach)\n",
    "    sentences = re.split(r'[.!?]+', text)\n",
    "    \n",
    "    em_dash_sentences = []\n",
    "    for sentence in sentences:\n",
    "        sentence = sentence.strip()\n",
    "        if sentence and has_em_dash(sentence):\n",
    "            em_dash_sentences.append(sentence+\".\")\n",
    "    \n",
    "    return em_dash_sentences\n",
    "\n",
    "def extract_em_dash_sentences_from_analyses(analyses_list):\n",
    "    \"\"\"Extract em dash sentences from a list of story analyses\"\"\"\n",
    "    all_em_dash_sentences = []\n",
    "    \n",
    "    for analysis in analyses_list:\n",
    "        story_text = analysis['story_text']\n",
    "        model = analysis['model']\n",
    "        \n",
    "        sentences = extract_sentences_with_em_dashes(story_text)\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            dash_count = count_em_dashes(sentence)\n",
    "            sentence_data = {\n",
    "                'sentence': sentence,\n",
    "                'model': model,\n",
    "                'total_dashes': dash_count['total_em_dashes'],\n",
    "                'em_dash_count': dash_count['em_dash_count'],\n",
    "                'en_dash_count': dash_count['en_dash_count'],\n",
    "                'double_hyphen_count': dash_count['double_hyphen_count']\n",
    "            }\n",
    "            all_em_dash_sentences.append(sentence_data)\n",
    "    \n",
    "    return all_em_dash_sentences\n",
    "\n",
    "print(\"\u2705 Em dash extraction functions ready!\")\n",
    "print(\"\ud83d\udd0d Functions available: extract_sentences_with_em_dashes, extract_em_dash_sentences_from_analyses\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21609eb",
   "metadata": {},
   "source": [
    "## 5. Paraphrase Generation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edadc193",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_word_difference(text1, text2):\n",
    "    \"\"\"\n",
    "    Calculate the number of tokens that differ between two texts using frequency-based comparison.\n",
    "    Includes both words and punctuation as tokens.\n",
    "    \"\"\"\n",
    "    import re\n",
    "    from collections import Counter\n",
    "    \n",
    "    def get_tokens(text):\n",
    "        \"\"\"Extract all tokens (words + punctuation) from text, normalizing em dashes\"\"\"\n",
    "        # Normalize different dash types to em dash for consistency\n",
    "        cleaned = re.sub(r'[\u2014\u2013]|--', '\u2014', text)\n",
    "        # Split into tokens: words (\\w+) and punctuation (\\S)\n",
    "        tokens = re.findall(r'\\w+|[^\\w\\s]', cleaned.lower())\n",
    "        return tokens\n",
    "    \n",
    "    tokens1 = get_tokens(text1)\n",
    "    tokens2 = get_tokens(text2)\n",
    "    \n",
    "    # Calculate differences using frequency counters\n",
    "    counter1 = Counter(tokens1)\n",
    "    counter2 = Counter(tokens2)\n",
    "    \n",
    "    # Calculate frequency-based differences\n",
    "    unique_to_text1 = sum((counter1 - counter2).values())\n",
    "    unique_to_text2 = sum((counter2 - counter1).values())\n",
    "    total_difference = unique_to_text1 + unique_to_text2\n",
    "    \n",
    "    return {\n",
    "        'total_difference': total_difference,\n",
    "        'tokens1': tokens1,\n",
    "        'tokens2': tokens2,\n",
    "        'unique_to_text1': unique_to_text1,\n",
    "        'unique_to_text2': unique_to_text2,\n",
    "        'similarity_ratio': 1 - (total_difference / max(len(tokens1), len(tokens2))) if max(len(tokens1), len(tokens2)) > 0 else 1\n",
    "    }\n",
    "\n",
    "def is_paraphrase_similar_enough(original, paraphrase, max_word_difference=4):\n",
    "    \"\"\"Check if paraphrase is similar enough to original\"\"\"\n",
    "    word_diff = calculate_word_difference(original, paraphrase)\n",
    "    return word_diff['total_difference'] <= max_word_difference\n",
    "\n",
    "def generate_paraphrase(sentence, model_name=\"gpt-35-turbo\"):\n",
    "    \"\"\"Generate a paraphrase that removes em dashes\"\"\"\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model_name,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a writing assistant. Rewrite sentences to remove em dashes (\u2014) while preserving the original meaning. Use alternative punctuation like commas, periods, or restructure the sentence. Keep the meaning and tone as close to the original as possible.\"},\n",
    "                {\"role\": \"user\", \"content\": f\"Rewrite this sentence to remove all em dashes while keeping the same meaning: {sentence}\"}\n",
    "            ],\n",
    "            max_tokens=150,\n",
    "            temperature=0.3\n",
    "        )\n",
    "        \n",
    "        if response and response.choices and len(response.choices) > 0:\n",
    "            content = response.choices[0].message.content\n",
    "            if content:\n",
    "                paraphrase = content.strip().replace('\"', '').replace(\"'\", \"\").strip()\n",
    "                # Ensure no em dashes remain\n",
    "                if '\u2014' not in paraphrase and '\u2013' not in paraphrase and '--' not in paraphrase:\n",
    "                    return paraphrase\n",
    "        \n",
    "        return None\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\u274c Error generating paraphrase: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def generate_multiple_paraphrases(sentence, model_name=\"gpt-35-turbo\", max_attempts=5, max_paraphrases=2):\n",
    "    \"\"\"Generate multiple paraphrases for a sentence\"\"\"\n",
    "    paraphrases = []\n",
    "    attempts = 0\n",
    "    \n",
    "    while len(paraphrases) < max_paraphrases and attempts < max_attempts:\n",
    "        attempts += 1\n",
    "        paraphrase = generate_paraphrase(sentence, model_name)\n",
    "        \n",
    "        if paraphrase and paraphrase not in paraphrases:\n",
    "            paraphrases.append(paraphrase)\n",
    "    \n",
    "    return paraphrases\n",
    "\n",
    "def process_em_dash_replacements_filtered(em_dash_sentences, max_word_difference=5, max_paraphrases_per_sentence=2):\n",
    "    \"\"\"Process em dash sentences and generate filtered paraphrases\"\"\"\n",
    "    print(f\"\ud83c\udfa8 GENERATING PARAPHRASES (\u2264{max_word_difference} token diff, up to {max_paraphrases_per_sentence} per sentence)\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    all_results = []\n",
    "    total_attempts = 0\n",
    "    successful_paraphrases = 0\n",
    "    filtered_out = 0\n",
    "    \n",
    "    for i, sentence_data in enumerate(em_dash_sentences):\n",
    "        original_sentence = sentence_data['sentence']\n",
    "        model = sentence_data['model']\n",
    "        \n",
    "        print(f\"\\n\ud83d\udd04 Processing {i+1}/{len(em_dash_sentences)}: {model.upper()}\")\n",
    "        print(f\"\ud83d\udcdd Original: \\\"{original_sentence[:60]}...\\\"\")\n",
    "        \n",
    "        # Generate multiple paraphrases\n",
    "        paraphrases = generate_multiple_paraphrases(\n",
    "            original_sentence, \n",
    "            model_name=\"gpt-35-turbo\",\n",
    "            max_paraphrases=max_paraphrases_per_sentence\n",
    "        )\n",
    "        \n",
    "        valid_paraphrases = []\n",
    "        \n",
    "        for paraphrase in paraphrases:\n",
    "            total_attempts += 1\n",
    "            \n",
    "            # Check similarity\n",
    "            if is_paraphrase_similar_enough(original_sentence, paraphrase, max_word_difference):\n",
    "                # Calculate token changes\n",
    "                original_tokens = count_tokens(original_sentence, model)\n",
    "                paraphrase_tokens = count_tokens(paraphrase, model)\n",
    "                token_difference = paraphrase_tokens['token_count'] - original_tokens['token_count']\n",
    "                \n",
    "                # Count dashes removed\n",
    "                original_dashes = count_em_dashes(original_sentence)['total_em_dashes']\n",
    "                paraphrase_dashes = count_em_dashes(paraphrase)['total_em_dashes']\n",
    "                dashes_removed = original_dashes - paraphrase_dashes\n",
    "                \n",
    "                valid_paraphrases.append({\n",
    "                    'paraphrase': paraphrase,\n",
    "                    'token_difference': token_difference,\n",
    "                    'dashes_removed': dashes_removed,\n",
    "                    'original_tokens': original_tokens['token_count'],\n",
    "                    'paraphrase_tokens': paraphrase_tokens['token_count']\n",
    "                })\n",
    "                \n",
    "                successful_paraphrases += 1\n",
    "                print(f\"\u2705 Valid: \\\"{paraphrase[:50]}...\\\" (tokens: {token_difference:+d})\")\n",
    "            else:\n",
    "                filtered_out += 1\n",
    "                print(f\"\u274c Filtered: Too different from original\")\n",
    "        \n",
    "        if valid_paraphrases:\n",
    "            result = {\n",
    "                'original_sentence': original_sentence,\n",
    "                'model': model,\n",
    "                'original_dashes': sentence_data['total_dashes'],\n",
    "                'paraphrases': valid_paraphrases,\n",
    "                'paraphrase_count': len(valid_paraphrases)\n",
    "            }\n",
    "            all_results.append(result)\n",
    "        \n",
    "        time.sleep(0.5)  # Rate limiting\n",
    "    \n",
    "    print(f\"\\n\u2705 PARAPHRASE GENERATION COMPLETE!\")\n",
    "    print(f\"\ud83d\udcca Results: {successful_paraphrases} valid paraphrases from {total_attempts} attempts\")\n",
    "    print(f\"\ud83d\udd0d Filtered out: {filtered_out} paraphrases (too different)\")\n",
    "    print(f\"\ud83d\udccb Sentences with valid paraphrases: {len(all_results)}\")\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "print(\"\u2705 Paraphrase generation functions ready!\")\n",
    "print(\"\ud83c\udfa8 Functions available: generate_paraphrase, process_em_dash_replacements_filtered\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac172cc",
   "metadata": {},
   "source": [
    "## 6. Pipeline Execution\n",
    "\n",
    "Execute the complete data generation pipeline in 3 steps:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611d86d6",
   "metadata": {},
   "source": [
    "### Step 1: Generate Stories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0b4b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \ud83c\udfad STEP 1: GENERATE STORIES AND SAVE TO JSON\n",
    "print(\"\ud83c\udfad GENERATING STORIES AND SAVING TO JSON\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Configuration\n",
    "num_story_rounds = 1  # Adjust as needed\n",
    "print(f\"\ud83d\udcca Configuration: {num_story_rounds} rounds \u00d7 {len(available_models)} models = {num_story_rounds * len(available_models)} stories\")\n",
    "print()\n",
    "\n",
    "# Generate stories\n",
    "print(\"\ud83d\ude80 Starting story generation...\")\n",
    "all_story_analyses = generate_multiple_stories_with_analysis(num_story_rounds)\n",
    "\n",
    "if all_story_analyses:\n",
    "    print(f\"\\n\u2705 Generated {len(all_story_analyses)} stories successfully!\")\n",
    "    \n",
    "    # Save to JSON\n",
    "    stories_filename = save_to_json(\n",
    "        all_story_analyses, \n",
    "        \"story_analyses\", \n",
    "        f\"{len(all_story_analyses)} story analyses\"\n",
    "    )\n",
    "    \n",
    "    # Display summary\n",
    "    print(f\"\\n\ud83d\udcca STORY GENERATION SUMMARY:\")\n",
    "    model_counts = {}\n",
    "    total_em_dashes = 0\n",
    "    \n",
    "    for analysis in all_story_analyses:\n",
    "        model = analysis['model']\n",
    "        model_counts[model] = model_counts.get(model, 0) + 1\n",
    "        total_em_dashes += analysis.get('total_em_dashes', 0)\n",
    "    \n",
    "    print(f\"   \u2022 Total stories: {len(all_story_analyses)}\")\n",
    "    print(f\"   \u2022 Total em dashes found: {total_em_dashes}\")\n",
    "    \n",
    "    for model, count in model_counts.items():\n",
    "        print(f\"   \u2022 {model.upper()}: {count} stories\")\n",
    "    \n",
    "    print(f\"\\n\ud83c\udfaf Ready for Step 2: Em dash extraction\")\n",
    "    print(f\"\ud83d\udcc1 Stories saved in: {stories_filename}\")\n",
    "    \n",
    "else:\n",
    "    print(\"\u274c No stories were generated!\")\n",
    "    print(\"\ud83d\udca1 Check API connection and try again\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7023b1aa",
   "metadata": {},
   "source": [
    "### Step 2: Extract Em Dashes and Generate Paraphrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e283a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to find latest file\n",
    "def find_latest_file(pattern):\n",
    "    \"\"\"Find the most recent file matching the pattern\"\"\"\n",
    "    import glob\n",
    "    files = glob.glob(pattern)\n",
    "    if files:\n",
    "        return max(files, key=os.path.getctime)\n",
    "    return None\n",
    "\n",
    "def load_from_json(filename, description=\"data\"):\n",
    "    \"\"\"Load data from JSON file\"\"\"\n",
    "    try:\n",
    "        with open(filename, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        print(f\"\u2705 {description} loaded from: {filename}\")\n",
    "        if isinstance(data, (list, dict)):\n",
    "            print(f\"\ud83d\udcdd Size: {len(data)} items\")\n",
    "        return data\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\u274c Error loading {description}: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2cd911",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \ud83d\udd0d STEP 2: LOAD STORIES, EXTRACT EM DASHES, GENERATE PARAPHRASES\n",
    "print(\"\ud83d\udd0d LOADING STORIES AND PROCESSING EM DASHES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Configuration for paraphrasing\n",
    "max_word_difference = 5\n",
    "max_paraphrases_per_sentence = 2\n",
    "\n",
    "print(f\"\ud83c\udfa8 Paraphrase config: \u2264{max_word_difference} token differences, up to {max_paraphrases_per_sentence} per sentence\")\n",
    "print(f\"\ud83d\udeab Em dash filter: Reject any paraphrases containing em dashes (\u2014)\")\n",
    "print()\n",
    "\n",
    "# Load from latest file automatically (or use the stories from Step 1 if still in memory)\n",
    "if 'all_story_analyses' not in locals() or not all_story_analyses:\n",
    "    latest_stories_file = find_latest_file(\"story_analyses_*.json\")\n",
    "    if latest_stories_file:\n",
    "        print(f\"\ud83d\udcc2 Loading from latest file: {latest_stories_file}\")\n",
    "        all_story_analyses = load_from_json(latest_stories_file, \"story analyses\")\n",
    "    else:\n",
    "        print(\"\u274c No story analysis files found!\")\n",
    "        print(\"\ud83d\udca1 Run Step 1 first to generate stories\")\n",
    "        all_story_analyses = None\n",
    "else:\n",
    "    print(\"\ud83d\udcc2 Using stories from Step 1 (still in memory)\")\n",
    "\n",
    "if all_story_analyses:\n",
    "    print(f\"\u2705 Loaded {len(all_story_analyses)} story analyses\")\n",
    "    \n",
    "    # Extract em dash sentences\n",
    "    print(\"\\n\ud83d\udd0d Extracting em dash sentences...\")\n",
    "    em_dash_sentences = extract_em_dash_sentences_from_analyses(all_story_analyses)\n",
    "    \n",
    "    if em_dash_sentences:\n",
    "        print(f\"\u2705 Found {len(em_dash_sentences)} sentences with em dashes\")\n",
    "        \n",
    "        # Generate paraphrases\n",
    "        print(f\"\\n\ud83c\udfa8 Generating paraphrases...\")\n",
    "        paraphrase_results = process_em_dash_replacements_filtered(\n",
    "            em_dash_sentences,\n",
    "            max_word_difference=max_word_difference,\n",
    "            max_paraphrases_per_sentence=max_paraphrases_per_sentence\n",
    "        )\n",
    "        \n",
    "        if paraphrase_results:\n",
    "            print(f\"\\n\u2705 Generated {len(paraphrase_results)} paraphrases successfully!\")\n",
    "            \n",
    "            # Save paraphrases to JSON\n",
    "            paraphrases_filename = save_to_json(\n",
    "                paraphrase_results,\n",
    "                \"paraphrase_results\",\n",
    "                f\"{len(paraphrase_results)} paraphrase results\"\n",
    "            )\n",
    "            \n",
    "            # Also save em dash sentences for reference\n",
    "            em_dash_filename = save_to_json(\n",
    "                em_dash_sentences,\n",
    "                \"em_dash_sentences\", \n",
    "                f\"{len(em_dash_sentences)} em dash sentences\"\n",
    "            )\n",
    "            \n",
    "            print(f\"\\n\ud83d\udcca PARAPHRASING SUMMARY:\")\n",
    "            print(f\"   \u2022 Em dash sentences: {len(em_dash_sentences)}\")\n",
    "            print(f\"   \u2022 Successful paraphrases: {len(paraphrase_results)}\")\n",
    "            print(f\"   \u2022 Average paraphrases per sentence: {len(paraphrase_results)/len(em_dash_sentences):.2f}\")\n",
    "            \n",
    "            print(f\"\\n\ud83c\udfaf Ready for Step 3: Final compilation\")\n",
    "            print(f\"\ud83d\udcc1 Paraphrases saved in: {paraphrases_filename}\")\n",
    "            print(f\"\ud83d\udcc1 Em dash sentences saved in: {em_dash_filename}\")\n",
    "            \n",
    "        else:\n",
    "            print(\"\u274c No paraphrases generated!\")\n",
    "            print(\"\ud83d\udca1 Try adjusting max_word_difference or generating more stories\")\n",
    "    else:\n",
    "        print(\"\u274c No em dash sentences found!\")\n",
    "        print(\"\ud83d\udca1 Try generating more stories to find em dashes\")\n",
    "else:\n",
    "    print(\"\u274c Could not load story analyses\")\n",
    "    print(\"\ud83d\udca1 Make sure Step 1 has been completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0dca81",
   "metadata": {},
   "source": [
    "### Step 3: Compile Complete Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1f238c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \ud83d\udcca STEP 3: COMPILE COMPLETE ANALYSIS\n",
    "print(\"\ud83d\udcca COMPILING COMPLETE ANALYSIS\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Load all data if not in memory\n",
    "print(\"\ud83d\udcc2 Loading all pipeline data...\")\n",
    "\n",
    "# Load stories (from memory or file)\n",
    "if 'all_story_analyses' not in locals() or not all_story_analyses:\n",
    "    latest_stories_file = find_latest_file(\"story_analyses_*.json\")\n",
    "    if latest_stories_file:\n",
    "        all_story_analyses = load_from_json(latest_stories_file, \"story analyses\")\n",
    "    else:\n",
    "        all_story_analyses = None\n",
    "\n",
    "# Load paraphrases (from memory or file)\n",
    "if 'paraphrase_results' not in locals() or not paraphrase_results:\n",
    "    latest_paraphrases_file = find_latest_file(\"paraphrase_results_*.json\")\n",
    "    if latest_paraphrases_file:\n",
    "        paraphrase_results = load_from_json(latest_paraphrases_file, \"paraphrase results\")\n",
    "    else:\n",
    "        paraphrase_results = None\n",
    "\n",
    "# Load em dash sentences (from memory or file)\n",
    "if 'em_dash_sentences' not in locals() or not em_dash_sentences:\n",
    "    latest_em_dash_file = find_latest_file(\"em_dash_sentences_*.json\")\n",
    "    if latest_em_dash_file:\n",
    "        em_dash_sentences = load_from_json(latest_em_dash_file, \"em dash sentences\")\n",
    "    else:\n",
    "        em_dash_sentences = None\n",
    "\n",
    "# Compile complete analysis if all data is available\n",
    "if all_story_analyses and paraphrase_results and em_dash_sentences:\n",
    "    print(f\"\\n\u2705 All pipeline data loaded successfully!\")\n",
    "    print(f\"   \u2022 Story analyses: {len(all_story_analyses)}\")\n",
    "    print(f\"   \u2022 Em dash sentences: {len(em_dash_sentences)}\")\n",
    "    print(f\"   \u2022 Paraphrase results: {len(paraphrase_results)}\")\n",
    "    \n",
    "    # Compile comprehensive results\n",
    "    complete_analysis_results = {\n",
    "        'story_analyses': all_story_analyses,\n",
    "        'em_dash_sentences': em_dash_sentences,\n",
    "        'paraphrase_results': paraphrase_results,\n",
    "        'pipeline_metadata': {\n",
    "            'generation_timestamp': datetime.now().isoformat(),\n",
    "            'total_stories': len(all_story_analyses),\n",
    "            'total_em_dash_sentences': len(em_dash_sentences),\n",
    "            'total_paraphrases': sum(len(result.get('paraphrases', [])) for result in paraphrase_results),\n",
    "            'models_used': available_models,\n",
    "            'pipeline_config': {\n",
    "                'max_word_difference': max_word_difference,\n",
    "                'max_paraphrases_per_sentence': max_paraphrases_per_sentence,\n",
    "                'num_story_rounds': num_story_rounds if 'num_story_rounds' in locals() else 'unknown'\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Save complete analysis results\n",
    "    analysis_filename = save_to_json(\n",
    "        complete_analysis_results,\n",
    "        \"complete_analysis\",\n",
    "        \"complete pipeline analysis\"\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n\ud83c\udf89 PIPELINE COMPLETED SUCCESSFULLY!\")\n",
    "    print(f\"\ud83d\udcca Final Statistics:\")\n",
    "    print(f\"   \u2022 Total stories generated: {len(all_story_analyses)}\")\n",
    "    print(f\"   \u2022 Total em dash sentences extracted: {len(em_dash_sentences)}\")\n",
    "    print(f\"   \u2022 Total paraphrases generated: {complete_analysis_results['pipeline_metadata']['total_paraphrases']}\")\n",
    "    \n",
    "    print(f\"\\n\ud83d\udcbe Complete analysis saved to: {analysis_filename}\")\n",
    "    print(f\"\ud83c\udfaf Ready for evaluation analysis!\")\n",
    "    print(f\"\ud83d\udca1 Use the evaluation_analysis.ipynb notebook to analyze the generated data\")\n",
    "    \n",
    "else:\n",
    "    print(\"\u274c Could not load all required pipeline data!\")\n",
    "    print(\"\ud83d\udca1 Make sure Steps 1 and 2 have been completed successfully\")\n",
    "    \n",
    "    # Show what's available\n",
    "    print(f\"\\n\ud83d\udcc1 Data availability:\")\n",
    "    print(f\"   \u2022 Story analyses: {'\u2705' if all_story_analyses else '\u274c'}\")\n",
    "    print(f\"   \u2022 Em dash sentences: {'\u2705' if em_dash_sentences else '\u274c'}\")\n",
    "    print(f\"   \u2022 Paraphrase results: {'\u2705' if paraphrase_results else '\u274c'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03253ff",
   "metadata": {},
   "source": [
    "## 7. Pipeline Summary\n",
    "\n",
    "\u2705 **Pipeline Complete!**\n",
    "\n",
    "This notebook has generated:\n",
    "- **Stories**: AI-generated stories from multiple models\n",
    "- **Em Dash Sentences**: Extracted sentences containing em dashes\n",
    "- **Paraphrases**: Alternative versions without em dashes\n",
    "- **Complete Analysis**: Combined dataset for evaluation\n",
    "\n",
    "**Output Files** (with timestamps):\n",
    "- `story_analyses_YYYYMMDD_HHMMSS.json`\n",
    "- `em_dash_sentences_YYYYMMDD_HHMMSS.json`\n",
    "- `paraphrase_results_YYYYMMDD_HHMMSS.json`\n",
    "- `complete_analysis_YYYYMMDD_HHMMSS.json`\n",
    "\n",
    "**Next Steps:**\n",
    "1. Use `evaluation_analysis.ipynb` to analyze the generated data\n",
    "2. Generate visualizations and statistics\n",
    "3. Conduct token change analysis\n",
    "4. Create publication-ready results\n",
    "\n",
    "---\n",
    "\n",
    "**Notes:**\n",
    "- Adjust `num_story_rounds` in Step 1 to generate more/fewer stories\n",
    "- Modify `max_word_difference` and `max_paraphrases_per_sentence` in Step 2 for different filtering\n",
    "- All intermediate data is saved, so you can re-run individual steps as needed"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiexpress",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}